Lab 10: Design and Implementation of Machine Learning Pipelines on Big Data with Spark Mllib
1.	CÃ i Ä‘áº·t pháº§n má»m cáº§n thiáº¿t
1.1.	Táº¡o project CS167
-	VÃ o C:/Users/<your-username-directory> -> Táº¡o thÆ° má»¥c má»›i vá»›i tÃªn : cs167
 

1.2.	CÃ i Ä‘áº·t JDK 1.8
-	Truy cáº­p vÃ o link: https://www.oracle.com/java/technologies/javase/javase8-archive-downloads.html
 
-	Down load â€œWindow x64 Installerâ€
-	Cháº¡y file vá»«a má»›i táº£i vá»
-	Nháº¥n â€œNextâ€
 
â€ƒ
-	Nháº¥n â€œChangeâ€
 
-	Chá»n Ä‘Æ°á»ng dáº«n tá»›i thÆ° má»¥c vá»«a táº¡o, thÃªm jdk vÃ o Ä‘Æ°á»ng dáº«n Folder name -> OK
 
 
-	Next -> Äá»£i quÃ¡ trÃ¬nh cÃ i Ä‘áº·t hoÃ n táº¥t -> Close









-	Cáº¥u hÃ¬nh Java home
 
 



-	Hiá»ƒn thá»‹ version Java-
-	JAVA -VERSION
-	JAVAC -VERSION

 
1.3.	CÃ i Ä‘áº·t Apache Maven
-	Truy cáº­p vÃ o link: https://maven.apache.org/download.cgi
 
-	Download Link â€œBinary zip archiveâ€
-	VÃ o directory chá»©a file zip vá»«a download
-	Chuá»™t pháº£i -> Extract All... -> Browse
 
-	Dáº«n tá»›i thÆ° má»¥c chá»©a lab, táº¡o thÆ° má»¥c má»›i vá»›i tÃªn maven:
 
 




-	Chá»n â€œExtractâ€
 
1.4.	CÃ i Ä‘áº·t IntelliJ IDEA Community Edition
-	Truy cáº­p vÃ o link: https://www.jetbrains.com/idea/download/?section=windows
-	Chá»n phiÃªn báº£n IntelliJ IDEA Community Edition -> Download
 
-	Cháº¡y file vá»«a download vá»
-	Nháº¥n  â€œNextâ€
 
-	Nháº¥n â€œNextâ€
 
-	Chá»n Add â€œbinâ€ folder to the PATH -> Next -> Install
 
-	Reboot now -> Finish
 
1.5.	CÃ i Ä‘áº·t biáº¿n mÃ´i trÆ°á»ng
1.5.1.	ThÃªm JAVA_HOME
-	Windows + R, nháº­p rundll32.exe sysdm.cpl,EditEnvironmentVariables -> OK
 
-	á» má»¥c User variable for <user-name>, New -> Nháº­p Variable name: JAVA_HOME -> Chá»n Browse Directory... -> chá»n tá»›i thÆ° má»¥c nÆ¡i chá»©a jdk Ä‘Ã£ cÃ i Ä‘áº·t trÆ°á»›c Ä‘Ã³ -> OK.
 
1.5.2.	ThÃªm MAVEN_HOME
-	TÆ°Æ¡ng tá»± vá»›i MAVEN_HOME
 
1.5.3.	ThÃªm java vÃ  maven vÃ o PATH
-	Chá»n Path
 
-	Nháº­p %JAVA_HOME%\bin vÃ  %MAVEN_HOME%\bin -> OK
 
-	Chá»n â€œOKâ€
 
1.5.4.	Kiá»ƒm tra mÃ´i trÆ°á»ng
-	Kiá»ƒm tra: vÃ o cmd -> gÃµ javac -version vÃ  mvn -version, náº¿u hiá»ƒn thá»‹ nhÆ° dÆ°á»›i áº£nh -> ThÃ nh cÃ´ng
 

1.6 CÃ i Ä‘áº·t hadoop
1.6.1 Táº£i vÃ  ThÃªm HADOOP_HOME
Hadoop yÃªu cáº§u Java Development Kit (JDK) Ä‘á»ƒ hoáº¡t Ä‘á»™ng. Náº¿u chÆ°a cÃ i thÃ¬ quay láº¡i bÆ°á»›c 1.2 
CÃ¡c bÆ°á»›c cÃ i Ä‘áº·t Hadoop bao gá»“m:
-	Táº£i xuá»‘ng phiÃªn báº£n Hadoop tá»« trang chá»§ cá»§a Apache Hadoop á»Ÿ link sau:
https://mirror.downloadvn.com/apache/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz
-	Giáº£i nÃ©n gÃ³i Hadoop vá»«a táº£i xuá»‘ng vÃ o á»• D.
-	Cáº¥u hÃ¬nh cÃ¡c táº­p tin cáº§n thiáº¿t Ä‘á»ƒ Hadoop hoáº¡t Ä‘á»™ng.
TÆ°Æ¡ng tá»± vá»›i cÃ¡c bÆ°á»›c setup JDK, thiáº¿t láº­p cÃ¡c biáº¿n vÃ  Ä‘Æ°á»ng dáº«n má»›i cho Hadoop
 
-	á» má»¥c â€œSystem variablesâ€ chá»n má»¥c â€œPathâ€ vÃ  nháº¥n â€œEditâ€.
 
-	Chá»n New vÃ  thÃªm cÃ¡c Ä‘Æ°á»ng dáº«n sau  â€œD:\hadoop\binâ€ vÃ  â€œD:\hadoop\sbinâ€
 

1.6.2 Cáº¥u hÃ¬nh cÃ¡c táº­p tin cho Hadoop
CÃ¡c táº­p tin cáº¥u hÃ¬nh chÃ­nh cá»§a Hadoop bao gá»“m:
â—	core-site.xml: Cáº¥u hÃ¬nh cÃ¡c thiáº¿t láº­p chung cho Hadoop.
â—	hdfs-site.xml: Cáº¥u hÃ¬nh cÃ¡c thiáº¿t láº­p cho HDFS.
â—	mapred-site.xml: Cáº¥u hÃ¬nh cÃ¡c thiáº¿t láº­p cho MapReduce.
â—	yarn-site.xml: Cáº¥u hÃ¬nh cÃ¡c thiáº¿t láº­p cho YARN (Yet Another Resource Negotiator).
â—	hadoop-env.cmd
-	Cáº¥u hÃ¬nh core-site.xml nhÆ° dÆ°á»›i Ä‘Ã¢y:
<configuration>
   <property>
       <name>fs.defaultFS</name>
       <value>hdfs://localhost:9000</value>
   </property>
</configuration>
-	Cáº¥u hÃ¬nh mapred-site.xml nhÆ° dÆ°á»›i Ä‘Ã¢y:
<configuration>
   <property>
       <name>mapreduce.framework.name</name>
       <value>yarn</value>
   </property>
</configuration>
-	Cáº¥u hÃ¬nh hdfs-site.xml nhÆ° dÆ°á»›i Ä‘Ã¢y:


-	Táº¡o thÆ° má»¥c â€œdataâ€ trong â€œD:\hadoop\hadoop-3.3.0\dataâ€
-	Táº¡o thÆ° má»¥c con â€œdatanodeâ€ trong â€œD:\hadoop\hadoop-3.3.0\dataâ€
-	Táº¡o thÆ° má»¥c con â€œnamenodeâ€ trong â€œD:\hadoop\hadoop-3.3.0\dataâ€



-	Sau Ä‘Ã³ cáº¥u hÃ¬nh hdfs-site.xml nhÆ° sau:
<configuration>
   <property>
       <name>dfs.replication</name>
       <value>1</value>
   </property>
   <property>
       <name>dfs.namenode.name.dir</name>
       <value>D:\hadoop\hadoop-hadoop-3.3.0\data\namenode</value>
   </property>
   <property>
       <name>dfs.datanode.data.dir</name>
       <value> D:\hadoop\hadoop-3.3.0\data\ datanode</value>
   </property>
</configuration>
ChÃº Ã½ : Do lÆ°u hadoop vÃ o á»• Ä‘Ä©a D nÃªn pháº§n <value></value> cÅ©ng thay Ä‘á»•i tÆ°Æ¡ng á»©ng
-	Cáº¥u hÃ¬nh yarn-site.xml nhÆ° dÆ°á»›i Ä‘Ã¢y:
<configuration>
   <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
   </property>
   <property>
               <name>yarn.nodemanager.auxservices.mapreduce.shuffle.class</name>
               <value>org.apache.hadoop.mapred.ShuffleHandler</value>
   </property>
</configuration>
-	Cáº¥u hÃ¬nh hadoop-env.cmd:
Má»Ÿ file nÃ y lÃªn vÃ  tÃ¬m tá»›i lá»‡nh:
â–ª	set JAVA_HOME=%JAVA_HOME%
sá»­a %JAVA_HOME% thÃ nh Ä‘Æ°á»ng dáº«n cÃ i JDK trong á»• C:
â–ª	set JAVA_HOME=C:\Progra~1\Java\jdk1.8.0_211
Sau Ä‘Ã³ format láº¡i namenode vÃ  datanode: 
Má»Ÿ command line lÃªn, gÃµ lá»‡nh sau:
â–ª	hdfs namenode â€“format
â–ª	hdfs datanode -format
 
    BÆ°á»›c format nÃ y chá»‰ cáº§n lÃ m 1 láº§n.

1.6.3 HoÃ n thÃ nh vÃ  cháº¡y thá»­ nghiá»‡m
GÃµ lá»‡nh start-all.cmd vÃ  jps Ä‘á»ƒ kiá»ƒm tra
 
Sau khi gÃµ lá»‡nh trÃªn, há»‡ thá»‘ng sáº½ cháº¡y Hadoop
Pháº£i Ä‘áº£m báº£o cÃ¡c á»©ng dá»¥ng sau Ä‘Æ°á»£c cháº¡y:
â€“ Hadoop Namenode
â€“ Hadoop datanode
â€“ YARN Resource Manager
â€“ YARN Node Manager
 
NhÆ° váº­y lÃ  ta Ä‘Ã£ khá»Ÿi cháº¡y thÃ nh cÃ´ng Hadoop.

1.6.	CÃ i Ä‘áº·t dá»± Ã¡n
-	Khá»Ÿi táº¡o dá»± Ã¡n: VÃ o IntelliJ -> New Project
 
-	Chá»n Maven Archetype -> Äiá»n cÃ¡c thÃ´ng tin nhÆ° Name, Location: lÆ°u trá»¯ project á»Ÿ thÆ° má»¥c cs167 Ä‘Ã£ táº¡o (chá»n JDK 1.8)
-	Chá»n jdk: 1.8
-	Catalog: Maven Cantral
-	Archetype: net.alchim31.maven:scala-archetype-simple
-	Version: 1.7
-	 
-	Chon create Ä‘á»ƒ táº¡o dá»± Ã¡n.
-	Má»Ÿ file pom.xml vÃ  bá»• sung thÃªm
 
-	Trong pom.xml, thay tháº¿ cÃ¡c thÃ nh pháº§n sau theo máº«u dÆ°á»›i Ä‘Ã¢y
o   properties:
-        <spark.version>3.4.4</spark.version>
 
o   dependencies:
-        <dependency>
   <groupId>org.apache.spark</groupId>
   <artifactId>spark-core_${scala.compat.version}</artifactId>
   <version>${spark.version}</version>
   <scope>compile</scope>
 </dependency>

 <dependency>
   <groupId>org.apache.spark</groupId>
   <artifactId>spark-mllib_${scala.compat.version}</artifactId>
   <version>${spark.version}</version>
 </dependency>

 <dependency>
   <groupId>org.apache.spark</groupId>
   <artifactId>spark-sql_${scala.compat.version}</artifactId>
   <version>${spark.version}</version>
 </dependency>
 
Thay tháº¿ org.scala-lang


<dependency>
  <groupId>org.scala-lang</groupId>
  <artifactId>scala-library</artifactId>
  <version>2.12.15</version>
</dependency>


-	BÃªn gÃ³c pháº£i -> chá»n biá»ƒu tÆ°á»£ng Maven -> Sync All Maven Projects
 
-	Náº¿u cÃ³ lá»—i xáº£y ra -> chá»n biá»ƒu tÆ°á»£ng Maven ->  Execute Maven Goal
 
-	GÃµ clean -> Chá»n mvn clean
 
-	Sau Ä‘Ã³ vÃ o láº¡i, gÃµ package -> Äá»£i cho Ä‘áº¿n khi load xong
 
-	VÃ o src -> main -> scala -> bigdata.group3 -> App.scala , thay tháº¿ toÃ n bá»™ báº±ng Ä‘oáº¡n code dÆ°á»›i Ä‘Ã¢y:
package edu.ucr.cs.cs167.nnlap10

import org.apache.spark.SparkConf
import org.apache.spark.ml.classification.{LinearSVC, LinearSVCModel}
import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator
import org.apache.spark.ml.feature.{HashingTF, StringIndexer, Tokenizer}
import org.apache.spark.ml.param.ParamMap
import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit, TrainValidationSplitModel}
import org.apache.spark.ml.{Pipeline, PipelineModel}
import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}

object App {

  def main(args : Array[String]) {
    if (args.length != 1) {
      println("Usage <input file>")
      println("  - <input file> path to a CSV file input")
      sys.exit(0)
    }
    val inputfile = args(0)
    val conf = new SparkConf
    if (!conf.contains("spark.master"))
      conf.setMaster("local[*]")
    println(s"Using Spark master '${conf.get("spark.master")}'")

    val spark = SparkSession
      .builder()
      .appName("CS167 Lab10")
      .config(conf)
      .getOrCreate()

    val t1 = System.nanoTime
    try {
      // process the sentiment data
      // TDOO A: read CSV file as a DataFrame
      //val sentimentData: DataFrame = // ...

      // TODO B: tokenize text (sentences) to array of words
      //val tokenzier = // ...

      // TODO C: transform words to numeric features using HashingTF
      //val hashingTF = // ...

      // TODO D: transform labels to numbers
      //val stringIndexer = // ...

      // TODO E: create an object for the Linear Support Vector Machine classifier
      //val svc = // ...

      // TODO F: create a pipeline that includes all the previous transofrmaitons and the model
      //val pipeline = // ...

      // TODO G: create a parameter grid to corss validate the model on different hyper parameters
      // val paramGrid: Array[ParamMap] = new ParamGridBuilder()
      //   .addGrid(/* ... */)
      //   .addGrid(/* ... */)
      //   .build()

      // TDOO H: create a corss validation job that will process the pipeline using all possible combinations in the parameter grid

      // TODO I: split the data into 80% train and 20% test

      // TODO J: Run cross-validation, and choose the best set of parameters.

      // TODO K: get the parameters of the best model and print them

      // TODO L: apply the model to your test set and show sample of the result

      // TODO M: evaluate the test results

      val t2 = System.nanoTime
      println(s"Applied sentiment analysis algorithm on input $inputfile in ${(t2 - t1) * 1E-9} seconds")
    } finally {
      spark.stop
    }
  }
}

-	Truy cáº­p vÃ o Ä‘Ã¢y Ä‘á»ƒ táº£i file sentiment.csv:
CS167/Labs/Lab10/sentiment.csv.zip at master Â· aseldawy/CS167
-	Chá»n Dowload raw file
 
-	Tiáº¿n hÃ nh giáº£i nÃ©n vÃ o trong folder cs167

-	Di chuyá»ƒn sentiment.csv vÃ o thÆ° má»¥c lab10 (nÆ¡i chá»©a project)
 
-	VÃ o App -> Edit configurations
 
-	á» má»¥c program arguments -> thÃªm sentiment.csv -> OK
 

-	Chá»n Apply -> OK
1.7.	Äá»c dá»¯ liá»‡u Ä‘áº§u vÃ o
1.7.1.	TODO A: Read CSV file as a DataFrame
-	ThÃªm Ä‘oáº¡n code dÆ°á»›i Ä‘Ã¢y:
val sentimentData: DataFrame = spark.read.format("csv")
  .option("header", "true")
  .option("quote", "\"")
  .option("escape", "\"")
  .load(inputfile)
sentimentData.printSchema()
sentimentData.show()

-	Nháº¥n vÃ o nÃºt Run Ä‘á»ƒ cháº¡y
 
-	Káº¿t quáº£:
 
1.8.	ÄÃ o táº¡o mÃ´ hÃ¬nh MÃ¡y vectÆ¡ há»— trá»£ trÃªn dá»¯ liá»‡u tÃ¬nh cáº£m
1.8.1.	TODO B: tokenize text (sentences) to array of words
val tokenizer = new Tokenizer()
  .setInputCol("text")
  .setOutputCol("words")
val tokenizedData = tokenizer.transform(sentimentData)
tokenizedData.select("text", "words").show()
 
1.8.2.	TODO C: transform words to numeric features using HashingTF
val hashingTF = new HashingTF()
  .setInputCol("words")
  .setOutputCol("features")
  .setNumFeatures(1024)
val featurizedData = hashingTF.transform(tokenizedData)
featurizedData.select("words", "features").show()
 
1.8.3.	TODO D: transform labels to numbers
val stringIndexer = new StringIndexer()
  .setInputCol("sentiment")
  .setOutputCol("label")
  .setHandleInvalid("skip")
val indexedData = stringIndexer.fit(featurizedData).transform(featurizedData)
indexedData.select("sentiment", "label").show()
 
1.8.4.	TODO E: create an object for the Linear Support Vector Machine classifier
val svc = new LinearSVC()
  .setFeaturesCol("features")
  .setLabelCol("label")
  .setMaxIter(10)
  .setRegParam(0.1)

1.8.5.	TODO F: create a pipeline that includes all the previous transformations and the model
val pipeline = new Pipeline()
  .setStages(Array(
    tokenizer,
    hashingTF,
    stringIndexer,
    svc))

1.8.6.	TODO G: create a parameter grid to cross validate the model on different hyper parameters
-	Äoáº¡n mÃ£ trong Scala sá»­ dá»¥ng thÆ° viá»‡n MLlib cá»§a Apache Spark Ä‘á»ƒ xÃ¢y dá»±ng táº­p há»£p cÃ¡c cáº¥u hÃ¬nh tham sá»‘ (param grid) cho quÃ¡ trÃ¬nh tuning mÃ´ hÃ¬nh â€” cá»¥ thá»ƒ lÃ  má»™t mÃ´ hÃ¬nh SVM (Support Vector Classification) dÃ¹ng LinearSVC.
val paramGrid: Array[ParamMap] = new ParamGridBuilder()
  .addGrid(hashingTF.numFeatures, Array(1024, 2048))
  .addGrid(svc.fitIntercept, Array(true, false))
  .addGrid(svc.regParam, Array(0.01, 0.0001))
  .addGrid(svc.maxIter, Array(10, 15))
  .addGrid(svc.threshold, Array(0.0, 0.25))
  .addGrid(svc.tol, Array(0.0001, 0.01))
  .build()

-	.addGrid(hashingTF.numFeatures, Array(1024, 2048)): sá»‘ lÆ°á»£ng (tÃ­nh nÄƒng) cá»¥ thá»ƒ Ä‘Æ°á»£c táº¡o bá»Ÿi HashingTF. Sá»‘ cÃ ng lá»›n â†’ mÃ´ hÃ¬nh cÃ ng cÃ³ nhiá»u thÃ´ng tin Ä‘á»ƒ há»c, nhÆ°ng cÅ©ng tá»‘n RAM vÃ  thá»i gian hÆ¡n
-	.addGrid(svc.fitIntercept, Array(true, false)): Cho biáº¿t cÃ³ tÃ­nh thÃªm há»‡ sá»‘ chá»‡ch (intercept) hay khÃ´ng. True: cÃ³ tÃ­nh â†’ mÃ´ hÃ¬nh linh hoáº¡t hÆ¡n, Ä‘áº·c biá»‡t náº¿u dá»¯ liá»‡u khÃ´ng cÃ¢n báº±ng; False: khÃ´ng tÃ­nh â†’ Ä‘Æ¡n giáº£n hÆ¡n nhÆ°ng cÃ³ thá»ƒ lÃ m giáº£m Ä‘á»™ chÃ­nh xÃ¡c náº¿u dá»¯ liá»‡u khÃ´ng cÃ¢n báº±ng quanh gá»‘c.
-	.addGrid(svc.regParam, Array(0.01, 0.0001)):ÄÃ¢y lÃ  há»‡ sá»‘ Ä‘iá»u chuáº©n (regularization) â€” giÃºp mÃ´ hÃ¬nh khÃ´ng há»c ká»¹ nÄƒng vÃ o dá»¯ liá»‡u huáº¥n luyá»‡n (overfitting).0.01 rÃ ng buá»™c máº¡nh hÆ¡n â†’ trÃ¡nh overfitting; 0.0001 rÃ ng buá»™c yáº¿u hÆ¡n â†’ mÃ´ hÃ¬nh dá»… há»c tá»‘t hÆ¡n trÃªn train, nhÆ°ng cÃ³ nguy cÆ¡ overfit.
-	.addGrid(svc.maxIter, Array(10, 15)): LÃ  sá»‘ vÃ²ng láº·p tá»‘i Ä‘a Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh. Náº¿u mÃ´ hÃ¬nh chÆ°a â€œhá»c xongâ€ trong 10 vÃ²ng â†’ cÃ³ thá»ƒ cáº§n tÄƒng lÃªn.
-	.addGrid(svc.threshold, Array(0.0, 0.25)): LÃ  ngÆ°á»¡ng phÃ¢n loáº¡i: xÃ¡c Ä‘á»‹nh tá»« bao nhiÃªu Ä‘iá»ƒm thÃ¬ mÃ´ hÃ¬nh gÃ¡n lÃ  nhÃ³m 1, cÃ²n láº¡i lÃ  nhÃ³m 0. 0.0 nháº¡y nháº¥t, gáº§n nhÆ° cÃ¡i gÃ¬ cÅ©ng gÃ¡n lÃ  nhÃ³m 1 ; 0.25 mÃ´ hÃ¬nh cáº§n cháº¯c cháº¯n hÆ¡n má»›i gÃ¡n lÃ  nhÃ³m 1.
-	.addGrid(svc.tol, Array(0.0001, 0.01)): ÄÃ¢y lÃ  ngÆ°á»¡ng dá»«ng: náº¿u mÃ´ hÃ¬nh há»c hoÃ n thÃ nh Ä‘áº¿n má»©c Ä‘á»™ chÃ­nh xÃ¡c nÃ y thÃ¬ nÃ³ sáº½ dá»«ng láº¡i. 0.0001 ráº¥t cháº·t â†’ cáº§n chÃ­nh xÃ¡c hÆ¡n má»›i dá»«ng (cháº­m hÆ¡n); 0.01 dá»… tÃ­nh hÆ¡n â†’ dá»«ng sá»›m hÆ¡n, nhanh hÆ¡n

1.8.7.	TODO H: create a cross validation job that will process the pipeline using all possible combinations in the parameter grid
val cv = new TrainValidationSplit()
  .setEstimator(pipeline)
  .setEvaluator(new BinaryClassificationEvaluator())
  .setEstimatorParamMaps(paramGrid)
  .setTrainRatio(0.8)
  .setParallelism(2)
TrainValidationSplit lÃ  má»™t ká»¹ thuáº­t Ä‘á»ƒ tÃ¬m kiáº¿m vÃ  chá»n ra mÃ´ hÃ¬nh tá»‘t nháº¥t tá»« má»™t táº­p há»£p cÃ¡c siÃªu tham sá»‘ (hyperparameters) Ä‘Ã£ cho. Thay vÃ¬ sá»­ dá»¥ng Cross Validation K-Fold phá»©c táº¡p, nÃ³ chá»‰ chia dá»¯ liá»‡u thÃ nh hai pháº§n: táº­p huáº¥n luyá»‡n (training set) vÃ  táº­p kiá»ƒm Ä‘á»‹nh (validation set).
.setTrainRatio(0.8):
â—	.setTrainRatio(): PhÆ°Æ¡ng thá»©c nÃ y xÃ¡c Ä‘á»‹nh tá»· lá»‡ chia dá»¯ liá»‡u giá»¯a táº­p huáº¥n luyá»‡n vÃ  táº­p kiá»ƒm Ä‘á»‹nh.
â—	0.8: NghÄ©a lÃ  80% dá»¯ liá»‡u Ä‘áº§u vÃ o sáº½ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh, vÃ  20% cÃ²n láº¡i (1 - 0.8) sáº½ Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m táº­p kiá»ƒm Ä‘á»‹nh Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i cÃ¡c bá»™ siÃªu tham sá»‘ khÃ¡c nhau.
.setParallelism(2):
â—	.setParallelism(): PhÆ°Æ¡ng thá»©c nÃ y Ä‘áº·t má»©c Ä‘á»™ song song khi huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh vá»›i cÃ¡c bá»™ siÃªu tham sá»‘ khÃ¡c nhau.
â—	2: NghÄ©a lÃ  TrainValidationSplit cÃ³ thá»ƒ huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ tá»‘i Ä‘a 2 mÃ´ hÃ¬nh vá»›i 2 bá»™ siÃªu tham sá»‘ khÃ¡c nhau cÃ¹ng má»™t lÃºc. Äiá»u nÃ y cÃ³ thá»ƒ tÄƒng tá»‘c quÃ¡ trÃ¬nh tÃ¬m kiáº¿m siÃªu tham sá»‘ náº¿u cÃ³ nhiá»u core hoáº·c tÃ i nguyÃªn sáºµn cÃ³.


1.8.8.	TODO I: split the data into 80% train and 20% test
val Array(trainingData: Dataset[Row], testData: Dataset[Row]) = sentimentData.randomSplit(Array(0.8, 0.2))

1.8.9.	TODO J: Run cross-validation, and choose the best set of parameters
val model: TrainValidationSplitModel = cv.fit(trainingData)

1.8.10.	TODO K: get the parameters of the best model and print them
val numFeatures: Int = model.bestModel.asInstanceOf[PipelineModel].stages(1).asInstanceOf[HashingTF].getNumFeatures
val fitIntercept: Boolean = model.bestModel.asInstanceOf[PipelineModel].stages(3).asInstanceOf[LinearSVCModel].getFitIntercept
val regParam: Double = model.bestModel.asInstanceOf[PipelineModel].stages(3).asInstanceOf[LinearSVCModel].getRegParam
val maxIter: Double = model.bestModel.asInstanceOf[PipelineModel].stages(3).asInstanceOf[LinearSVCModel].getMaxIter
val threshold: Double = model.bestModel.asInstanceOf[PipelineModel].stages(3).asInstanceOf[LinearSVCModel].getThreshold
val tol: Double = model.bestModel.asInstanceOf[PipelineModel].stages(3).asInstanceOf[LinearSVCModel].getTol
println("Best parameters of the best model:")
println(s"numFeatures: $numFeatures")
println(s"fitIntercept: $fitIntercept")
println(f"regParam: $regParam%.4f")
println(f"maxIter: $maxIter%.4f")
println(f"threshold: $threshold%.4f")
println(f"tol: $tol%.4f");

1.8.11.	TODO L: apply the model to your test set and show sample of the result
val predictions: DataFrame = model.transform(testData)
predictions.select("text", "sentiment", "label", "prediction").show()
 
1.8.12.	TODO M: evaluate the test results
val binaryClassificationEvaluator = new BinaryClassificationEvaluator()
  .setLabelCol("label")
  .setRawPredictionCol("prediction")
val accuracy: Double = binaryClassificationEvaluator.evaluate(predictions)
println(s"Accuracy of the test set is $accuracy")

ğŸ¡º	Result:
 
 

ğŸ¡º	(Q1) Fill the following table
The parameter values and the accuracy are based on the best model you obtained. The run time is the total run time printed by the program.
Parameter	Value
numFeatures	2048
fitIntercept	false
regParam	0.0001
maxIter	10.0000
threshold	0.2500
tol	 0.0100
Test accuracy	0.8164619490497372
Run time	123.395227 seconds

1.9.	Cháº¿ Ä‘á»™ phÃ¢n tÃ¡n
1.9.1.	CÃ i Ä‘áº·t Spark
-	Truy cáº­p vÃ o link Ä‘á»ƒ táº£i spark https://archive.apache.org/dist/spark/spark-3.4.4/spark-3.4.4-bin-hadoop3.tgz
-	Sau khi táº£i thÃ¬ giáº£i nÃ©n file vÃ o C:\spark-3.4.4
-	ThÃªm SPARK_HOME vÃ o environment variables
 
-	ThÃªm %SPARK_HOME%\bin vÃ o Path
 
-	Má»Ÿ command line vÃ  cháº¡y: spark-submit --version Ä‘á»ƒ kiá»ƒm tra cáº¥u hÃ¬nh Ä‘Ãºng
 
-	VÃ o thÆ° má»¥c conf -> copy file spark-defaults.conf.template -> dÃ¡n ra vÃ  Ä‘á»•i tÃªn thÃ nh spark-defaults.conf
 
-	Má»Ÿ file spark-defaults.conf, edit Ä‘oáº¡n code sau vÃ  lÆ°u file:
-	# spark.master                     spark://class-###:7077




Táº¡o file jar
VÃ o file chá»n Project Structure.
Hoáº·c Ctrl + Alt + Shift + S
 
Khi cá»­a sá»• hiá»‡n lÃªn chá»n vÃ o artifacts chá»n vÃ o dáº¥u +
Jar -> Form modules with â€¦.
Cá»­a sá»• hiá»‡n lÃªn chá»n main class chá»n class App
 
Nhá»› chá»n include in project build
 
Chá»n Apply vÃ  thoÃ¡t cá»­a sá»•
 
Tiáº¿p theo
Chá»n Build -> Build artifacts ->chá»n file jar cáº§n build vÃ  chá»n hÃ nh Ä‘á»™ng phÃ¹ há»£p
File jar Ä‘Æ°á»£c táº¡o ra vÃ  lÆ°u á»Ÿ out/artifacts/â€¦
TÃ¬m file lap10.jar
out/artifacts/lap10_jar/lap10.jar
1.9.2.	Khá»Ÿi cháº¡y Spark Cluster
-	Má»Ÿ 1 cá»­a sá»• CMD/Powershell Ä‘á»ƒ cháº¡y Master:
"%SPARK_HOME%\bin\spark-class.cmd" org.apache.spark.deploy.master.Master --host localhost --port 7077 --webui-port 8080
LÆ°u Ã½: Cá»•ng 8080 sáº½ tá»± thay Ä‘á»•i náº¿u cá»•ng 8080 Ä‘ang Ä‘Æ°á»£c sá»­ dá»¥ng
 
-	Má»Ÿ cá»­a sá»• CMD khÃ¡c Ä‘á»ƒ cháº¡y Worker:
"%SPARK_HOME%\bin\spark-class.cmd" org.apache.spark.deploy.worker.Worker spark://localhost:7077
LÆ°u Ã½: CÃ³ thá»ƒ cháº¡y nhiá»u worker báº±ng cÃ¡ch giá»›i háº¡n core cá»§a má»—i worker:
"%SPARK_HOME%\bin\spark-class.cmd"  org.apache.spark.deploy.worker.Worker --webui-port 8081 --cores 2 --memory 2G spark://localhost:7077
-	LÃªn trÃ¬nh diá»‡t nháº­p URL: http://localhost:8080, Spark Master Web UI vÃ  thÃ´ng tin Worker Ä‘Ã£ káº¿t ná»‘i.
 
-	Put file sentiment.csv lÃªn hdfs (lÆ°u Ã½ Ä‘Æ°á»ng dáº«n Ä‘áº¿n file sentiment.csv):
Má»Ÿ cmd run as administrator 
Cháº¡y láº§n lÆ°á»£t cÃ¡c lá»‡nh
Cd C:\hadoop-3.3.0\sbin
Start-all.cmd
Cd C:\Users\ADMIN\cs167
hadoop fs -rm -r /input
hadoop fs -mkdir /input
hdfs dfs -put sentiment.csv /input
â—	Lá»—i khi cháº¡y hadoop fs -rm -r /input
â—	Cháº¡y hdfs dfsadmin -safemode leave vÃ  cháº¡y láº¡i hadoop fs -rm -r /input
 
Check xem file Ä‘Ã£ Ä‘Æ°á»£c put chÆ°a: http://localhost:9870/
 
-	Cháº¡y lá»‡nh:
C:\spark-3.4.4/bin/spark-submit.cmd --master [Spark Master] --class [Class name] --name lap10_  [url file Jar] hdfs://localhost:9000/input/sentiment.csv
[Spark Master] : spark://localhost:7077
 
[Class name] : edu.ucr.cs.cs167.nnlap10.App
 
[url file Jar] :C:\Users\ADMIN\cs167\workspace\lap10\out\artifacts\lap10_jar2\lap10.jar 
 
C:\spark-3.4.4/bin/spark-submit.cmd --master spark://localhost:7077 --class edu.ucr.cs.cs167.nnlap10.App --name lap10_ C:\Users\ADMIN\cs167\workspace\lap10\out\artifacts\lap10_jar2\lap10.jar hdfs://localhost:9000/input/sentiment.csv

 
LÆ°u Ã½: Äá»ƒ console chá»‰ hiá»ƒn thá»‹ káº¿t quáº£ mÃ  khÃ´ng cáº§n hiá»ƒn thá»‹ nhá»¯ng log khÃ´ng cáº§n thiáº¿t, hÃ£y thá»±c hiá»‡n cÃ¡c bÆ°á»›c sau:
-	Táº¡o file â€œlog4j.propertiesâ€
 
-	ThÃªm Ä‘oáº¡n code sau vÃ o file:
log4j.rootCategory=ERROR, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%m%n


-	Sá»­ dá»¥ng file csv Ä‘Ã£ put lÃªn hdfs Ä‘á»ƒ cháº¡y:
C:\spark-3.4.4/bin/spark-submit.cmd --conf "spark.driver.extraJavaOptions=-Dlog4j.configuration=C:\Users\ADMIN\cs167\workspace\lap10\src\main\resources\log4j.properties"  --master spark://localhost:7077 --class edu.ucr.cs.cs167.nnlap10.App --name lap10_ C:\Users\ADMIN\cs167\workspace\lap10\out\artifacts\lap10_jar2\lap10.jar hdfs://localhost:9000/input/sentiment.csv
ğŸ¡º	(Q2) Fill the following table
ğŸ¡º	 
ğŸ¡º	 
Parameter	Value
Number of worker nodes in your cluster	2
Total number of cores in your cluster	18
numFeatures	2048
fitIntercept	true
regParam	0.0100
maxIter	10.0000
threshold	0.2500
tol	 0.0001
Test accuracy	0.813147719924573
Run time	 174.0333499000 seconds

-	Äá»ƒ stop spark master vÃ  spark worker dÃ¹ng lá»‡nh: jps
 
ChÃºng ta sáº½ tháº¥y cÃ¡c PID cá»§a Master vÃ  Worker, Ä‘á»ƒ stop dÃ¹ng lá»‡nh taskkill sau (lÆ°u Ã½ kill Worker trÆ°á»›c rá»“i Master sau Ä‘á»ƒ trÃ¡nh gáº·p lá»—i):
taskkill /PID 23316 /F
taskkill /PID 21884 /F
taskkill /PID 18680 /F
ğŸ¡º	(Q3) What difference do you notice in terms of the best parameters selected, the accuracy, and the run time between running the program locally and on your Spark cluster having multiple nodes?
-	Best parameters: gáº§n nhÆ° giá»‘ng nhau
-	Äá»™ chÃ­nh xÃ¡c: khÃ´ng chÃªnh lá»‡ch nhiá»u
-	Thá»i gian cháº¡y: local nhanh hÆ¡n Spark cluster

